\relax 
\citation{clark1996using}
\citation{palagi2008sharing}
\citation{knoblich2008evolving}
\citation{haroush2015neuronal}
\citation{thomas1995illusion}
\citation{baron1985does}
\citation{de2015savvy}
\citation{de2014theory}
\citation{de2011advantage}
\citation{meijering2011know}
\citation{pynadath2011modeling}
\citation{de2014theory}
\citation{weerd2015negotiating}
\citation{meijering2011know}
\citation{de2014agent}
\citation{pynadath2011modeling}
\citation{gmytrasiewicz1995rigorous}
\citation{clark1991grounding}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\citation{sutton1998reinforcement}
\citation{sequeira14}
\citation{puterman1994}
\citation{watkins1992q}
\citation{ng2000algorithms}
\citation{jin2011convergence}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mutual modeling}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model of itself}{\thepage }}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Q-learning. \textit  {TD} stands for \textit  {Temporal Difference}.}}{\thepage }}
\newlabel{algo1}{{1}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Model of others}{\thepage }}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Intuitive on-line IRL. Agent $i$ is inferring the reward function of agent $j$.}}{\thepage }}
\newlabel{algo2}{{2}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Model of itself seen by others}{\thepage }}
\citation{singh2010intrinsically}
\citation{de2008putting}
\@writefile{toc}{\contentsline {section}{\numberline {3}Expressing intentions}{\thepage }}
\newlabel{3}{{3}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Empathy and gratitude}{\thepage }}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Intuitive on-line IRL. Agent $i$ helps agent $j$ to infer $i$'s reward function.}}{\thepage }}
\newlabel{algo3}{{3}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Prisoner's dilemma}{\thepage }}
\citation{sandholm1996multiagent}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces IPD payoff matrix}}{\thepage }}
\newlabel{payoff}{{1}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results and discussion}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Pure Q-learning}{\thepage }}
\newlabel{61}{{6.1}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Q-learning with empathy \& gratitude}{\thepage }}
\newlabel{62}{{6.2}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta =0$}. Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. We can see that with small variances agents successfully learned that the other has negative reward $\mathcal  {S}$, but since the other was always defecting at the end, both thought that the other had strong positive reward $\mathcal  {P}$ that was, in fact, null (see yellow cells).}}{\thepage }}
\newlabel{R_00_00_Q}{{2}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta =0$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. (red) +/- standard deviation.}}{\thepage }}
\newlabel{00_00_Q}{{1}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta =0.3$}. Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. We can see that with small variances agents successfully learned that the other has negative reward $\mathcal  {S}$, but since the other was always defecting at the end, both thought that the other had strong positive reward $\mathcal  {P}$ that was, in fact, null. Furthermore, they also learned that the other was punished while they were both cooperating and receiving reward $\mathcal  {R}$ (see yellow cells).}}{\thepage }}
\newlabel{R_90_90_Q}{{3}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta =0.3$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. (red) +/- standard deviation.}}{\thepage }}
\newlabel{90_90_Q}{{2}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \leavevmode {\color  {red}$\alpha =-0.9$, $\beta = -0.3$}. Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. We can see that with small variances agents successfully learned that the other has negative reward $\mathcal  {S}$, but since the other was always defecting at the end, both thought that the other had strong positive reward $\mathcal  {P}$ that was, in fact, null (see yellow cells).}}{\thepage }}
\newlabel{R_m9_m9_Q}{{4}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Expressing intentions with empathy \& gratitude}{\thepage }}
\newlabel{63}{{6.3}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \leavevmode {\color  {red}$\alpha =-0.9$, $\beta =-0.3$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. (red) +/- standard deviation.}}{\thepage }}
\newlabel{m9_m9_Q}{{3}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta = 0$}. Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. Agents could learn each other's intentions and understood that $\mathcal  {T}$ and $\mathcal  {R}$ are positive rewards for the other. As they finally always cooperated (because of empathy), they estimated other's rewards higher for $\mathcal  {R}$ than for $\mathcal  {T}$(see yellow cells).}}{\thepage }}
\newlabel{R_90_90_B}{{5}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta =0$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. (red) +/- standard deviation.}}{\thepage }}
\newlabel{90_90_B}{{4}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Playing with empathy}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta = 0.9$}. (blue) Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. We can see that with small variances agents successfully learned that the other has negative reward $\mathcal  {S}$, but since the other was always defecting at the end, both thought that the other had strong positive reward $\mathcal  {P}$ that was, in fact, null (see yellow cells).}}{\thepage }}
\newlabel{R_09_09_B}{{6}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta =0.9$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. (red) +/- standard deviation.}}{\thepage }}
\newlabel{09_09_B}{{5}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{\thepage }}
\bibstyle{abbrv}
\bibdata{biblio}
\bibcite{baron1985does}{1}
\bibcite{clark1996using}{2}
\bibcite{clark1991grounding}{3}
\bibcite{de2008putting}{4}
\bibcite{de2015savvy}{5}
\bibcite{de2014agent}{6}
\bibcite{de2014theory}{7}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta = 0.3$}. Average learned other's reward function by agents 1 and 2 over 50 IPD games and variances. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. Agents could learn each other's intentions and understood that $\mathcal  {T}$ and $\mathcal  {R}$ are positive rewards for the other. As they finally always cooperated (because of empathy), they estimated other's rewards higher for $\mathcal  {R}$ than for $\mathcal  {T}$(see yellow cells).}}{\thepage }}
\newlabel{R_99_99_B}{{7}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \leavevmode {\color  {red}$\alpha =0.9$, $\beta =0.3$}. (blue) Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represents a full cooperation (both agents cooperate) and -1 represents a full defection (both agents defect). The trajectory is computed with an exponential moving average of this ratio. Between times $t=301$ and $t=700$ agents were following expressing-intentions behavior. (red) +/- standard deviation.}}{\thepage }}
\newlabel{99_99_B}{{6}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average final defect-cooperate ratio over 10 IDP games for a grid of 400 possible ($\alpha _1$, $\alpha _2$) combinations. In each game, agents were adopting expressing-intentions behavior between time $t=301$ and $t=700$. Red areas correspond to combinations that led to cooperation while blue areas correspond to combinations that led to Nash equilibrium. In green areas, agents were equally defecting and cooperating.}}{\thepage }}
\newlabel{alphamap}{{7}{\thepage }}
\bibcite{de2011advantage}{8}
\bibcite{gmytrasiewicz1995rigorous}{9}
\bibcite{haroush2015neuronal}{10}
\bibcite{jin2011convergence}{11}
\bibcite{knoblich2008evolving}{12}
\bibcite{meijering2011know}{13}
\bibcite{ng2000algorithms}{14}
\bibcite{palagi2008sharing}{15}
\bibcite{puterman1994}{16}
\bibcite{pynadath2011modeling}{17}
\bibcite{sandholm1996multiagent}{18}
\bibcite{sequeira14}{19}
\bibcite{singh2010intrinsically}{20}
\bibcite{sutton1998reinforcement}{21}
\bibcite{thomas1995illusion}{22}
\bibcite{watkins1992q}{23}
\bibcite{weerd2015negotiating}{24}
