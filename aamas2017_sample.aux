\relax 
\citation{meijering2011know}
\citation{de2015savvy}
\citation{weerd2015negotiating}
\citation{de2014agent}
\citation{de2014theory}
\citation{de2011advantage}
\citation{pynadath2011modeling}
\citation{de2014theory}
\citation{weerd2015negotiating}
\citation{meijering2011know}
\citation{de2014agent}
\citation{pynadath2011modeling}
\citation{gmytrasiewicz1995rigorous}
\citation{clark1991grounding}
\citation{sequeira14}
\citation{puterman1994}
\citation{watkins1992q}
\citation{ng2000algorithms}
\citation{jin2011convergence}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mutual modeling}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Non-recursive approach}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Model of itself}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Model of others}{\thepage }}
\citation{de2008putting}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Model of itself seen by others}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}Expressing intentions}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Empathy and gratitude}{\thepage }}
\citation{sandholm1996multiagent}
\@writefile{toc}{\contentsline {section}{\numberline {5}Prisoner's dilemma}{\thepage }}
\newlabel{PDmatrix}{{5}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces IPD payoff matrix}}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results and Discussion}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Pure Q-learning}{\thepage }}
\newlabel{R_00_00_Q}{{6.1}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta =0$}. Average learned other's rewards functions by agents 1 and 2 over 50 IPD games and variances. We can see that with a small variance agents successfully learned that the other has negative reward $\mathcal  {S}$, but as the other was always defecting at the end, both thought that the other had strong positive reward $\mathcal  {P}$ that is, in fact, null (see yellow cells).}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \leavevmode {\color  {red}$\alpha =0$, $\beta =0$}. Average trajectory of defect-cooperate ratio over 50 IPD games and variances. +1 represent a full cooperation (both agents cooperate) and -1 represent a full defection (both agents defect). the trajectory is computed with an exponential moving average of this ratio.}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Q-learning with empathy \& gratitude}{\thepage }}
\bibstyle{abbrv}
\bibdata{biblio}
\bibcite{clark1991grounding}{1}
\bibcite{de2008putting}{2}
\bibcite{de2015savvy}{3}
\bibcite{de2014agent}{4}
\bibcite{de2014theory}{5}
\bibcite{de2011advantage}{6}
\bibcite{gmytrasiewicz1995rigorous}{7}
\bibcite{jin2011convergence}{8}
\bibcite{meijering2011know}{9}
\newlabel{R_90_90_Q}{{6.2}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces $\alpha =0.9$, $\beta = 0.3$}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $\alpha =0.9$, $\beta = 0.3$}}{\thepage }}
\newlabel{R_90_90_Q}{{6.2}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces $\alpha =-0.9$, $\beta = -0.3$}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Expressing intentions with empathy \& gratitude}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Playing with empathy}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $\alpha =-0.9$, $\beta = -0.3$}}{\thepage }}
\newlabel{R_90_90_B}{{6.3}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces $\alpha =0.9$, $\beta = 0$, expressing intentions}}{\thepage }}
\bibcite{ng2000algorithms}{10}
\bibcite{puterman1994}{11}
\bibcite{pynadath2011modeling}{12}
\bibcite{sandholm1996multiagent}{13}
\bibcite{sequeira14}{14}
\bibcite{watkins1992q}{15}
\bibcite{weerd2015negotiating}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces $\alpha =0.9$, $\beta = 0$, expressing intentions}}{\thepage }}
\newlabel{R_90_90_B}{{6.3}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces $\alpha =0.9$, $\beta = 0$, expressing intentions}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces $\alpha =0.$, $\beta = 0.9$, expressing intentions}}{\thepage }}
\newlabel{R_90_90_B}{{6.3}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces $\alpha =0.9$, $\beta = 0$, expressing intentions}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $\alpha =0.9$, $\beta = 0.3$, expressing intentions}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces $\alpha $ varying}}{\thepage }}
